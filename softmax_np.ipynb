{"cells":[{"cell_type":"markdown","metadata":{},"source":["\n","<br>\n","This file is for fashion mnist classification<br>\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from util import get_mnist_data\n","from logistic_np import add_one, LogisticClassifier\n","# import time\n","#import pdb"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["class SoftmaxClassifier(LogisticClassifier):\n","    def __init__(self, w_shape):\n","        \"\"\"__init__\n","        \n","        :param w_shape: create w with shape w_shape using normal distribution\n","        \"\"\"\n","        super(SoftmaxClassifier, self).__init__(w_shape)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["    def softmax(self, x):\n","        \"\"\"softmax\n","        Compute softmax on the second axis of x\n","    \n","        :param x: input\n","        \"\"\"\n","        # [TODO 2.3]\n","        # Compute softmax\n","        z =  np.dot(x,self.w)\n","        z_max = np.empty((z.shape[0],1))\n","        z_max = np.amax(z, axis = 1)\n","        z_max = np.tile(z_max, (10,1)).T\n","        z_max = z - z_max\n","        r = np.exp(z_max)\n","        # for i in range(z.shape[0]):\n","        #     a = np.max(z[i,:])\n","        #     for j in range(z.shape[1]):\n","        #         z[i,j] = np.exp(z[i,j]-a)\n","        return r"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["    def feed_forward(self, x):\n","        \"\"\"feed_forward\n","        This function compute the output of your softmax regression model\n","        \n","        :param x: input\n","        \"\"\"\n","        # [TODO 2.3]\n","        # Compute a feed forward pass\n","        x = self.softmax(x)\n","        s = np.sum(x,axis=1)\n","        for i in range(x.shape[0]):\n","            x[i,:] = x[i,:]/s[i]\n","        return x"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["    def compute_loss(self, y, y_hat):\n","        \"\"\"compute_loss\n","        Compute the loss using y (label) and y_hat (predicted class)\n","        :param y:  the label, the actual class of the samples\n","        :param y_hat: the class probabilities of all samples in our data\n","        \"\"\"\n","        # [TODO 2.4]\n","        # Compute categorical loss\n","        for i in range (y.shape[0]):\n","            for j in range (y.shape[1]):\n","                y[i,j] = np.float32(y[i,j]*np.log(y_hat[i,j]) )\n","        # print(y)\n","        return -1/(y.shape[0])*np.sum(np.sum(y,axis = 1))   \n","        "]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["    def get_grad(self, x, y, y_hat):\n","        \"\"\"get_grad\n","        Compute and return the gradient of w\n","        :param loss: computed loss between y_hat and y in the train dataset\n","        :param y_hat: predicted y\n","        \"\"\" \n","        # [TODO 2.5]\n","        # Compute gradient of the loss function with respect to w\n","        grad = np.dot(x.T,(y_hat-y))\n","        return grad/x.shape[0]"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def plot_loss(train_loss, val_loss):\n","    plt.figure(1)\n","    plt.clf()\n","    plt.plot(train_loss, color='b')\n","    plt.plot(val_loss, color='g')"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["def draw_weight(w):\n","    label_names = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n","    plt.figure(2, figsize=(8, 6))\n","    plt.clf()\n","    w = w[0:(28*28),:].reshape(28, 28, 10)\n","    for i in range(10):\n","        ax = plt.subplot(3, 4, i+1)\n","        plt.imshow(w[:,:,i], interpolation='nearest')\n","        plt.axis('off')\n","        ax.set_title(label_names[i])"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def normalize(train_x, val_x, test_x):\n","    \"\"\"normalize\n","    This function computes train mean and standard deviation on all pixels then applying data scaling on train_x, val_x and test_x using these computed values\n","    Note that in this classification problem, the data is already flatten into a shape of (num_samples, image_width*image_height)\n","    :param train_x: train images, shape=(num_train, image_height*image_width)\n","    :param val_x: validation images, shape=(num_val, image_height*image_width)\n","    :param test_x: test images, shape=(num_test, image_height*image_width)\n","    \"\"\"\n","    # [TODO 2.1]\n","    # train_mean and train_std should have the shape of (1, 1)\n","    train_mean = np.mean(train_x) #single value\n","    train_std = np.std(train_x)\n","    train_x = (train_x - train_mean)/train_std\n","    val_x = (val_x - train_mean)/train_std\n","    test_x = (test_x - train_mean)/train_std\n","    # print(self.w_shape)\n","    return train_x,val_x,test_x"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["def create_one_hot(labels, num_k=10):\n","    \"\"\"create_one_hot\n","    This function creates a one-hot (one-of-k) matrix based on the given labels\n","    :param labels: list of labels, each label is one of 0, 1, 2,... , num_k - 1\n","    :param num_k: number of classes we want to classify\n","    \"\"\"\n","    # [TODO 2.2]\n","    # Create the one-hot label matrix here based on labels\n","    one_hot_labels = np.zeros((labels.shape[0],num_k))\n","    z = 0\n","    for i in labels:\n","        one_hot_labels[z,i] = 1\n","        z += 1\n","    return one_hot_labels"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["def test(y_hat, test_y):\n","    \"\"\"test\n","    Compute the confusion matrix based on labels and predicted values \n","    :param classifier: the trained classifier\n","    :param y_hat: predicted probabilites, output of classifier.feed_forward\n","    :param test_y: test labels\n","    \"\"\"\n","    \n","    confusion_mat = np.zeros((10,10))\n","    \n","    # [TODO 2.7]\n","    # Compute the confusion matrix here\n","    y_hat_label = np.argmax(y_hat,axis=1)\n","    test_y_label = np.argmax(test_y,axis=1)\n","    for i in range(10):\n","        for j in range(10):\n","            confusion_mat[i,j] = np.intersect1d(np.array(np.where(test_y_label==i)),np.array(np.where(y_hat_label==j))).shape[0]\n","    confusion_mat = confusion_mat/np.sum(confusion_mat,axis=1)\n","    #confusion_mat = confusion_mat/np.sum(confusion_mat,axis=1)\n","    np.set_printoptions(precision=2)\n","    print('Confusion matrix:')\n","    print(confusion_mat)\n","    print('Diagonal values:')\n","    print(confusion_mat.flatten()[0::11])"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading fashion MNIST data...\n","<class 'numpy.ndarray'>\n","Done reading\n","Confusion matrix:\n","[[0.93 0.02 0.   0.03 0.   0.   0.04 0.   0.   0.  ]\n"," [0.02 0.93 0.   0.07 0.   0.   0.   0.   0.   0.  ]\n"," [0.04 0.   0.61 0.05 0.23 0.   0.08 0.   0.   0.  ]\n"," [0.02 0.03 0.02 0.8  0.07 0.   0.02 0.   0.   0.  ]\n"," [0.   0.02 0.09 0.05 0.74 0.   0.08 0.   0.   0.  ]\n"," [0.   0.   0.   0.   0.   0.89 0.   0.1  0.02 0.  ]\n"," [0.24 0.   0.11 0.03 0.23 0.   0.45 0.   0.   0.  ]\n"," [0.   0.   0.   0.   0.   0.04 0.   0.84 0.   0.12]\n"," [0.   0.   0.   0.03 0.   0.04 0.   0.   0.94 0.  ]\n"," [0.   0.   0.   0.   0.   0.02 0.   0.02 0.   0.96]]\n","Diagonal values:\n","[0.93 0.93 0.61 0.8  0.74 0.89 0.45 0.84 0.94 0.96]\n"]}],"source":["if __name__ == \"__main__\":\n","    np.random.seed(2020)\n","\n","    # Load data from file\n","    # Make sure that fashion-mnist/*.gz files is in data/\n","    train_x, train_y, val_x, val_y, test_x, test_y = get_mnist_data()\n","    num_train = train_x.shape[0]\n","    num_val = val_x.shape[0]\n","    num_test = test_x.shape[0]  \n","\n","    #generate_unit_testcase(train_x.copy(), train_y.copy()) \n","\n","    # Convert label lists to one-hot (one-of-k) encoding\n","    train_y = create_one_hot(train_y)\n","    val_y = create_one_hot(val_y)\n","    test_y = create_one_hot(test_y)\n","\n","    # Normalize our data\n","    train_x, val_x, test_x = normalize(train_x, val_x, test_x)\n","    \n","    # Pad 1 as the last feature of train_x and test_x\n","    train_x = add_one(train_x) \n","    val_x = add_one(val_x)\n","    test_x = add_one(test_x)\n","    \n","    # Create classifier\n","    num_feature = train_x.shape[1]\n","    dec_classifier = SoftmaxClassifier((num_feature, 10))\n","    momentum = np.zeros_like(dec_classifier.w)\n","\n","    # Define hyper-parameters and train-related parameters\n","    num_epoch = 3347\n","    learning_rate = 0.01\n","    momentum_rate = 0.9\n","    epochs_to_draw = 10\n","    all_train_loss = []\n","    all_val_loss = []\n","    plt.ion()\n","    for e in range(num_epoch):    \n","        # tic = time.clock()\n","        train_y_hat = dec_classifier.feed_forward(train_x)\n","        val_y_hat = dec_classifier.feed_forward(val_x)\n","        train_loss = dec_classifier.compute_loss(train_y, train_y_hat)\n","        val_loss = dec_classifier.compute_loss(val_y, val_y_hat)\n","        grad = dec_classifier.get_grad(train_x, train_y, train_y_hat)\n","       \n","        # dec_classifier.numerical_check(train_x, train_y, grad)\n","        # Updating weight: choose either normal SGD or SGD with momentum\n","        dec_classifier.update_weight(grad, learning_rate)\n","        #dec_classifier.update_weight_momentum(grad, learning_rate, momentum, momentum_rate)\n","        all_train_loss.append(train_loss) \n","        all_val_loss.append(val_loss)\n","        # toc = time.clock()\n","        # print(toc-tic)\n","        # [TODO 2.6]\n","        # print(val_loss)\n","        # Propose your own stopping condition here\n","        best_val_loss = float(\"inf\")\n","        patience = 6  # Number of consecutive increases in validation loss before stopping\n","        consecutive_increases = 0\n","        if val_loss[-1] < best_val_loss:\n","            best_val_loss = val_loss\n","                # Optionally, save the model's parameters\n","                \n","        else:\n","            consecutive_increases += 1\n","        \n","        if consecutive_increases >= patience:\n","            print(\"Stopped due to overfitting.\")\n","            break\n","    y_hat = dec_classifier.feed_forward(test_x)\n","    # print(y_hat)\n","    test(y_hat, test_y)"]}],"metadata":{"kernelspec":{"display_name":"Python (py39)","language":"python","name":"py39"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":2}
