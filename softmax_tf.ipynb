{"cells":[{"cell_type":"markdown","metadata":{},"source":["\n","<br>\n","This file is for multiclass fashion-mnist classification using TensorFlow<br>\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from util import get_mnist_data\n","from logistic_np import add_one\n","from softmax_np import *\n","# import time\n","tf.compat.v1.disable_eager_execution() # for some reason, this line of code is required, need further investigation"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading fashion MNIST data...\n","<class 'numpy.ndarray'>\n","Done reading\n","Confusion matrix:\n","[[0.82 0.03 0.02 0.03 0.   0.   0.09 0.   0.02 0.  ]\n"," [0.   0.97 0.   0.05 0.   0.   0.   0.   0.   0.  ]\n"," [0.   0.03 0.67 0.   0.21 0.   0.08 0.   0.   0.  ]\n"," [0.05 0.02 0.04 0.78 0.   0.   0.06 0.   0.   0.  ]\n"," [0.   0.02 0.11 0.03 0.72 0.   0.09 0.   0.   0.  ]\n"," [0.   0.   0.   0.   0.   0.91 0.   0.08 0.   0.02]\n"," [0.16 0.02 0.13 0.05 0.16 0.   0.51 0.   0.02 0.  ]\n"," [0.   0.   0.   0.   0.   0.04 0.   0.9  0.   0.06]\n"," [0.02 0.   0.   0.03 0.05 0.02 0.02 0.02 0.87 0.  ]\n"," [0.   0.   0.   0.   0.   0.02 0.   0.   0.   0.98]]\n","Diagonal values:\n","[0.82 0.97 0.67 0.78 0.72 0.91 0.51 0.9  0.87 0.98]\n"]}],"source":["if __name__ == \"__main__\":\n","    np.random.seed(2020)\n","    tf.compat.v1.set_random_seed(2020)\n","\n","    # Load data from file\n","    # Make sure that fashion-mnist/*.gz files is in data/\n","    train_x, train_y, val_x, val_y, test_x, test_y = get_mnist_data()\n","    num_train = train_x.shape[0]\n","    num_val = val_x.shape[0]\n","    num_test = test_x.shape[0]  \n","\n","    # generate_unit_testcase(train_x.copy(), train_y.copy()) \n","\n","    # Convert label lists to one-hot (one-of-k) encoding\n","    train_y = create_one_hot(train_y)\n","    val_y = create_one_hot(val_y)\n","    test_y = create_one_hot(test_y)\n","\n","    # Normalize our data\n","    train_x, val_x, test_x = normalize(train_x, val_x, test_x)\n","    \n","    # Pad 1 as the last feature of train_x and test_x\n","    train_x = add_one(train_x) \n","    val_x = add_one(val_x)\n","    test_x = add_one(test_x)\n","   \n","    # [TODO 2.8] Create TF placeholders to feed train_x and train_y when training\n","    x = tf.compat.v1.placeholder(tf.float32, shape = [None,train_x.shape[1]], name = 'input')\n","    y = tf.compat.v1.placeholder(tf.float32, shape = [None,train_y.shape[1]], name ='output')\n","\n","    # [TODO 2.8] Create weights (W) using TF variables \n","    w =  tf.Variable(tf.zeros([train_x.shape[1],train_y.shape[1]]))\n","\n","    # [TODO 2.9] Create a feed-forward operator \n","    z = tf.matmul(x,w)\n","    # print(z.shape)\n","    a = tf.math.reduce_max(z,axis = 1)\n","    a = tf.reshape(a,(-1,1))\n","    b = tf.constant([1,10],dtype=tf.int32)\n","    a = tf.tile(a,b)\n","    # print(a)\n","    pred = tf.exp(z-a)\n","    # print(pred)\n","\n","    # [TODO 2.10] Write the cost function\n","    # cost = x.T@(y_hat - y)/x.shape[0]\n","    cost = -tf.reduce_sum(tf.reduce_sum(y*tf.math.log(pred),axis=1))/num_train\n","\n","    # Define hyper-parameters and train-related parameters\n","    num_epoch = 10000\n","    # num_epoch = 3347\n","    learning_rate = 0.01    \n","\n","    # [TODO 2.8] Create an SGD optimizer\n","    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n","    # Some meta parameters\n","    epochs_to_draw = 10\n","    all_train_loss = []\n","    all_val_loss = []\n","    plt.ion()\n","    num_val_increase = 0\n","\n","    # Start training\n","    init = tf.compat.v1.global_variables_initializer()\n","    \n","    with tf.compat.v1.Session() as sess:\n","        sess.run(init)\n","        for e in range(num_epoch):\n","            # tic = time.clock()\n","            # [TODO 2.8] Compute losses and update weights here\n","            train_loss = sess.run([cost], feed_dict={x:train_x, y:train_y}) \n","            val_loss = sess.run([cost], feed_dict={x:val_x, y:val_y}) \n","            # Update weights\n","            sess.run([optimizer], feed_dict={x: train_x, y: train_y})\n","            all_train_loss.append(train_loss)\n","            all_val_loss.append(val_loss)\n","            # print(val_loss)\n","            # toc = time.clock()\n","            # print(toc-tic)\n","            # [TODO 2.11] Define your own stopping condition here \n","            best_val_loss = float(\"inf\")\n","            patience = 6  # Number of consecutive increases in validation loss before stopping\n","            consecutive_increases = 0\n","            if val_loss[-1] < best_val_loss:\n","                best_val_loss = val_loss\n","                # Optionally, save the model's parameters\n","                \n","            else:\n","                consecutive_increases += 1\n","            \n","            if consecutive_increases >= patience:\n","                print(\"Stopped due to overfitting.\")\n","                break\n","        \n","        y_hat = sess.run(pred, feed_dict={x: test_x})\n","        test(y_hat, test_y)"]}],"metadata":{"kernelspec":{"display_name":"Python (py39)","language":"python","name":"py39"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":2}
